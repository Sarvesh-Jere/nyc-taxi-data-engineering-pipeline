# NYC Taxi Data Engineering Pipeline ğŸš–

This project demonstrates a real-time data engineering pipeline for processing NYC taxi trip data using modern, industry-grade tools. It simulates ingestion, processing, storage, orchestration, and analysis â€” reflecting how streaming and batch pipelines are built in production.

---

## ğŸ“Œ Key Concepts

This pipeline is built on the **microservices architecture**, where each component runs independently in a container and communicates via APIs or messaging systems. This makes the system modular, scalable, and easier to debug or deploy.

---

##  Project Overview

**Goal:** Stream and process NYC taxi trip data in real time, store it in a structured database, and enable analysis via notebooks.

---

## âš™ï¸ Tools Used

- **Apache Kafka** â€“ Real-time data ingestion (streaming events)
- **Apache Spark (Structured Streaming)** â€“ Real-time ETL and transformation
- **PostgreSQL** â€“ Relational storage for processed data
- **Apache Airflow** â€“ Workflow orchestration and scheduling
- **Docker + Docker Compose** â€“ Containerized environment
- **Jupyter Notebook** â€“ For exploratory data analysis and visualization

---

## ğŸ”„ Project Flow

1. **Kafka Producer**
   - Reads taxi trip records (simulated).
   - Publishes messages to the Kafka topic `rides`.

2. **Spark Structured Streaming Job**
   - Subscribes to the Kafka topic.
   - Transforms and parses incoming trip records.
   - Inserts the clean data into a PostgreSQL table.

3. **PostgreSQL Database**
   - Stores structured taxi trip data.
   - Serves as the analytical data store for downstream reporting.

4. **Airflow DAG**
   - Automates and schedules Spark jobs.
   - Can monitor ingestion and transformation tasks.

5. **Jupyter Notebook**
   - Connects to PostgreSQL.
   - Performs analysis on taxi rides (e.g., trip counts, average fare by zone, etc.).

---

##  Project Structure

nyc-taxi-data-engineering-pipeline/
â”œâ”€â”€ README.md # This file - you are here :)
â”œâ”€â”€ docker-compose.yml # Multi-container setup for the pipeline
â”œâ”€â”€ kafka-producer/
â”‚ â””â”€â”€ producer.py # Script to send messages to Kafka
â”œâ”€â”€ spark-job/
â”‚ â””â”€â”€ spark_streaming.py # Spark job to consume and process Kafka data
â”œâ”€â”€ airflow/
â”‚ â””â”€â”€ dags/
â”‚ â””â”€â”€ taxi_pipeline_dag.py # DAG to schedule pipeline jobs
â”œâ”€â”€ db/
â”‚ â””â”€â”€ init.sql # PostgreSQL schema and table setup
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ analysis.ipynb # EDA and visualization notebook

Connect With Me

If you found this project insightful, feel free to â­ the repo and connect with me on LinkedIn - https://www.linkedin.com/in/sarvesh-jere/
